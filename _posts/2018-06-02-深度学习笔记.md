---
layout: post
title: "深度学习笔记"
date: 2018-06-02
categories: 机器学习 Python
tags: 机器学习 Python
author: Quan Zhang
---

* content
{:toc}

## 神经网络的编程基础

### 二分类

假如有一张图片作为输入，比如一只猫，如果识别这张图为猫，则输出标签1，否则为0。

这张图片需要保存3个矩阵，对应RGB三个颜色。把这些像素值提取出来，放到一个特征向量x中，x为列向量。

符号定义：

- $x$：表示一个$n_x$维数据，为输入数据，维度为($n_x$,1)
- y：表示输出结果，取值0和1
- ($x^{(i)},y^{(i)}$)：表示第$i$组数据，训练数据(默认)或者测试数据
- $X=[x^{(1)},x^{(2)}...x^{(m)}]$：表示输入，$n_x*m$维矩阵，$m$为样本数目
- $Y=[y^{(1)},y^{(2)}...y^{(m)}]$：表示输出，$1*m$维，一行一列

执行X.shape，则输出($n_x,m$)，Y.shape，则输出($1,m$)。

![](/images/blog/20180602/1.jpg)

### 向量化

向量化可以避免使用for循环，可以加速程序运行：$z=np.dot(w,x)+b$。for循环执行时间可能比向量化慢300倍。

### 激活函数

1. sigmoid函数$\frac{1}{1+e^{-z}}$：除了输出层是一个二分类问题基本不会用它

2. tanh函数$\frac{e^z-e^{-z}}{e^z+e^{-z}}$：非常优秀，几乎适合所有场合

3. Relu函数$max(0,z)$：常用的默认函数

### 激活函数的导数

1. sigmoid：$d(g(z))=g(z)(1-g(z))$

2. tanh：$d(g(z))=1-(tanh(z))^2$

3. Relu：

<center>\[d(g(z))=\begin{cases}
& 0 \text { if } z<0\\ 
& 1 \text{ if } z>0\\
& ? \text { if } z=0 
\end{cases}\]</center>


## 利用TensorFlow寻找J最小的w

$J(w)=w^2-10w+25=(w-5)^2$，则J最小时，w=5

请测试你是否拥有TensorFlow的运行环境：

```python
import numpy as np
import tensorflow as tf
hello = tf.constant("Hello Tensotflow!")
sess = tf.Session()
print(sess.run(hello))
```

    b'Hello Tensotflow!'

寻求最小的J对应的w：
    
```python
import numpy as np
import tensorflow as tf
# 定义参数w
w = tf.Variable(0,dtype = tf.float32)
# 定义损失函数
cost = tf.add(tf.add(w**2,tf.multiply(-10.,w)),25)
# 用0.01的学习率，目标是最小化损失
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)
init = tf.global_variables_initializer()
session = tf.Session()
# 初始化全局变量
session.run(init)
print(session.run(w))# 输出为0
session.run(train)
print(session.run(w))# 输出约为0.1
# 执行1000次迭代，输出约为4.9999，接近J最小时w=5
for i in range(1000):
    session.run(train)
print(session.run(w))
```

    0.0
    0.099999994
    4.9999886
    
其实TensorFlow冲在了一般的加减法运算，可以把cost写成更直接的形式：

```python
# cost = tf.add(tf.add(w**2,tf.multiply(-10.,w)),25)
cost = w**2-10*w+25
```

加入训练集数据x，取代数字1，-10，25：

1. 定义x接入的数据：
    coefficients = np.array([[1.],[-10.],[25.]])

2. 定义x：
    x = tf.placeholder(tf.float32,[3,1])

3. 定义损失函数：
    cost = x[0][0]*w**2+x[1][0]*w+x[2][0]

4. 在训练中把数据接入x：
    session.run(train,feed_dict={x:coefficients})


```python
import numpy as np
import tensorflow as tf
# 定义x接入的数据
coefficients = np.array([[1.],[-10.],[25.]])
# 定义参数w
w = tf.Variable(0,dtype = tf.float32)
# 定义x
x = tf.placeholder(tf.float32,[3,1])
# 定义损失函数
# cost = tf.add(tf.add(w**2,tf.multiply(-10.,w)),25)
cost = x[0][0]*w**2+x[1][0]*w+x[2][0]
# 用0.01的学习率，目标是最小化损失
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)
init = tf.global_variables_initializer()
session = tf.Session()
# 初始化全局变量
session.run(init)
print(session.run(w))# 输出为0
session.run(train,feed_dict={x:coefficients})
print(session.run(w))# 输出约为0.1
# 执行1000次迭代，输出约为4.9999，接近J最小时w=5
for i in range(1000):
    session.run(train,feed_dict={x:coefficients})
print(session.run(w))
```

    0.0
    0.099999994
    4.9999886
    
