---
layout: post
title: "深度学习笔记"
date: 2018-06-02
categories: 机器学习 Python
tags: 机器学习 Python
author: Quan Zhang
---

* content
{:toc}

## 神经网络的编程基础

### 二分类

假如有一张图片作为输入，比如一只猫，如果识别这张图为猫，则输出标签1，否则为0。

这张图片需要保存3个矩阵，对应RGB三个颜色。把这些像素值提取出来，放到一个特征向量x中，x为列向量。

符号定义：

- $x$：表示一个$n_x$维数据，为输入数据，维度为($n_x$,1)
- y：表示输出结果，取值0和1
- ($x^{(i)},y^{(i)}$)：表示第$i$组数据，训练数据(默认)或者测试数据
- $X=[x^{(1)},x^{(2)}...x^{(m)}]$：表示输入，$n_x*m$维矩阵，$m$为样本数目
- $Y=[y^{(1)},y^{(2)}...y^{(m)}]$：表示输出，$1*m$维，一行一列

执行X.shape，则输出($n_x,m$)，Y.shape，则输出($1,m$)。

![](/images/blog/20180602/1.jpg)

### 向量化

向量化可以避免使用for循环，可以加速程序运行：$z=np.dot(w,x)+b$。for循环执行时间可能比向量化慢300倍。

### 激活函数

1. sigmoid函数$\frac{1}{1+e^{-z}}$：除了输出层是一个二分类问题基本不会用它

2. tanh函数$\frac{e^z-e^{-z}}{e^z+e^{-z}}$：非常优秀，几乎适合所有场合

3. Relu函数$max(0,z)$：常用的默认函数

### 激活函数的导数

1. sigmoid：$d(g(z))=g(z)(1-g(z))$

2. tanh：$d(g(z))=1-(tanh(z))^2$

3. Relu：

<center>\[d(g(z))=\begin{cases}
& 0 \text { if } z<0\\ 
& 1 \text{ if } z>0\\
& ? \text { if } z=0 
\end{cases}\]</center>


## 利用TensorFlow寻找J最小的w

$J(w)=w^2-10w+25=(w-5)^2$，则J最小时，w=5

请测试你是否拥有TensorFlow的运行环境：

```python
import numpy as np
import tensorflow as tf
hello = tf.constant("Hello Tensotflow!")
sess = tf.Session()
print(sess.run(hello))
```

    b'Hello Tensotflow!'

寻求最小的J对应的w：
    
```python
import numpy as np
import tensorflow as tf
# 定义参数w
w = tf.Variable(0,dtype = tf.float32)
# 定义损失函数
cost = tf.add(tf.add(w**2,tf.multiply(-10.,w)),25)
# 用0.01的学习率，目标是最小化损失
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)
init = tf.global_variables_initializer()
session = tf.Session()
# 初始化全局变量
session.run(init)
print(session.run(w))# 输出为0
session.run(train)
print(session.run(w))# 输出约为0.1
# 执行1000次迭代，输出约为4.9999，接近J最小时w=5
for i in range(1000):
    session.run(train)
print(session.run(w))
```

    0.0
    0.099999994
    4.9999886
    
其实TensorFlow冲在了一般的加减法运算，可以把cost写成更直接的形式：

```python
# cost = tf.add(tf.add(w**2,tf.multiply(-10.,w)),25)
cost = w**2-10*w+25
```

加入训练集数据x，取代数字1，-10，25：

1. 定义x接入的数据：
    coefficients = np.array([[1.],[-10.],[25.]])

2. 定义x：
    x = tf.placeholder(tf.float32,[3,1])

3. 定义损失函数：
    cost = x[0][0]*w**2+x[1][0]*w+x[2][0]

4. 在训练中把数据接入x：
    session.run(train,feed_dict={x:coefficients})


```python
import numpy as np
import tensorflow as tf
# 定义x接入的数据
coefficients = np.array([[1.],[-10.],[25.]])
# 定义参数w
w = tf.Variable(0,dtype = tf.float32)
# 定义x
x = tf.placeholder(tf.float32,[3,1])
# 定义损失函数
# cost = tf.add(tf.add(w**2,tf.multiply(-10.,w)),25)
cost = x[0][0]*w**2+x[1][0]*w+x[2][0]
# 用0.01的学习率，目标是最小化损失
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)
init = tf.global_variables_initializer()
session = tf.Session()
# 初始化全局变量
session.run(init)
print(session.run(w))# 输出为0
session.run(train,feed_dict={x:coefficients})
print(session.run(w))# 输出约为0.1
# 执行1000次迭代，输出约为4.9999，接近J最小时w=5
for i in range(1000):
    session.run(train,feed_dict={x:coefficients})
print(session.run(w))
```

    0.0
    0.099999994
    4.9999886
    
## 机器学习策略

当你构建一个深度学习系统时，不知道最终结果是好是坏，或许白费几个月的时间，所以需要一些有效的方法预先判断哪些想法是靠谱的。

### 正交化

如同调电视机一样，不同按键对应不同功能，才能达到最终的目的。搭建机器学习系统时，可以尝试和改变的东西太多，有太多的参数可以调，你要知道，对于调整什么来达到某个效果。

### 单一数字评估指标

无论你是调整超参数还是尝试不同的学习算法，如果你有一个单一的评估指标，你的进展会快很多。

对于分类器，一个合理的评估方式是查准率和查全率的折中表达：

<center>$F1=\frac{2PR}{P+R}$</center>

### 优化指标

准确率和时间往往是矛盾的。但可以设定一个阈值，比如在100ms内，不管时间多少，只要准确率高，时间都在接受范围内。

### 开发集和测试集大小

对于100个小样本，采用7/3或者6/2/2是合理的；对于100万的大样本，98/1/1已经够了。

### 迁移学习

在深度学习中，最强大的理念之一就是，有的时候神经网络可以从一个任务中学习，并将学得的知识应用到另一个独立的任务中。

### 多任务学习

在迁移学习中，你的步骤是串行的，你从任务A里学习只是然后迁移到任务B。在多任务学习中，你是同时开始学习的，试图让单个神经网络同时做几件事情，然后希望这里每个任务都能帮到其他所有任务。

对于一幅图$x^{(i)}$，可用4个标签$y^{(i)}$：有无行人，有无车，有无信号灯，有无交通标志。然后同时寻找图中的这四个标签。

**对于小数据集，用迁移学习；对于大数据集，用多任务学习。**

### 端到端学习

省去流水线的开发流程，直接用别人的训练集，学到x和y之间的函数映射。如果你的数据集很小，传统方法表现可能很好；但如果你的数据集很大，端到端方法或许是一个好方法。

## 卷积神经网络

### 三维卷积

彩色图像如果是6x6x3，用一个3x3x3的三维卷积核来卷积，最后的输出为4x4x1的图像：

![](/images/blog/20180602/2.jpg)

实际上就是计算一个3x3x3立方体三个分层的卷积和，可以把绿色和蓝色层卷积核设置为0，单独交算红色。

### 池化层

除了卷积层，卷积网络也经常使用池化层来缩减模型的大小，提高计算速度，同时提高所提取特征的鲁棒性。

#### 最大池化

假如输入为4x4矩阵，用到的池化类型为最大池化，树池是2x2矩阵，输出的元素为对应区域的最大值：

![](/images/blog/20180602/3.jpg)

![](/images/blog/20180602/4.jpg)

#### 平均池化

所谓平均池化，就是取每个区域的平均值，但不常用。