---
layout: post
title: "Pthon实现逻辑回归算法"
date: 2018-05-22
categories: 机器学习 Python
tags: 机器学习 Python
author: Quan Zhang
---

* content
{:toc}

这里将实现逻辑回归算法，并将之应用于两个数据集：[Logistic_data1.txt](https://zhangquan1995.github.io/res/20180522/Logistic_data1.txt)和[Logistic_data2.txt](https://zhangquan1995.github.io/res/20180522/Logistic_data2.txt)

需要实现的函数：

- plot_data: 绘制二维的分类数据
- sigmoid: sigmoid函数
- cost_function: 逻辑回归的代价函数
- predict: 逻辑回归的预测函数
- cost_function_reg: 逻辑回归带正则化项的代价函数

## 数据可视化

```python
    pos = y == 1
    neg = y == 0
    plt.plot(X[pos, 0], X[pos, 1], 'c+', label="Admitted")
    plt.plot(X[neg, 0], X[neg, 1], 'mo', label="Not admitted")

```

 ![](/images/blog/20180522/1.png)

## sigmoid函数

 逻辑回归的假设模型：

 <center>\[h_{\theta}(\pmb{x}) = g(\pmb{\theta}^{\mathrm{T}} \pmb{x})\]</center>

 其中函数<span>$g(·)$</span>是sigmoid函数，定义为：

 <center>\[g(z) = \frac{1}{1 + \exp(-z)}\]</center>

## 代价函数与梯度

现在需要实现逻辑函数回归的代价函数及其梯度。逻辑回归的代价函数为：

<center>\[J(\theta) = \frac{1}{m} \sum_{i=1}^{m} [ -y^{(i)} \log ( h_{\theta}(x^{(i)}) ) - (1-y^{(i)}) \log ( 1-h_{\theta}(x^{(i)}) ) ]\]</center>

对应的梯度向量各分量为：

<center>\[\frac{\partial J(\theta)}{\partial \theta_{j}} = \frac{1}{m} \sum_{i=1}^{m} ( h_{\theta}(x^{(i)}) - y^{(i)} ) x_{j}^{(i)}\]</center>

传入初始参数，cost_function的代价约为0.693。

## 使用fmin_cg学习模型参数

使用scipy.optimize.fmin_cg函数实现代价函数<span>$J(\theta)$</span>的优化，得到最佳参数<span>$\theta^{*}$</span>。


调用该函数的方法如下：

```python
    ret = op.fmin_cg(cost_function,
        theta_initial,
        fprime = cost_gradient,
        args = (X,y),
        maxiter = 200,
        full_output = True)

```

其中cost_function为代价函数，theta_initial为需要优化的参数的初始值，fprime=cost_gradient给出了代价函数的梯度，args=(X，y)给出了需要优化的函数与对应的梯度计算所需要的其他参数，maxiter=400给出了最大迭代次数，full_output=True则指明该函数除了输出优化得到的参数theta_opt外，还会返回最小的代价函数值cost_min等内容。对一组参数得到的代价约为0.203(cost_min)。

![](/images/blog/20180522/2.png)


## 评估逻辑回归模型

在获得模型参数后，可以使用模型预测一个学生能够被大学录取的几率。如果某学生考试一的成绩为45，考试二的成绩为85，能够得到录取几率约为0.776。

predict函数输出”1”或”0”，通过计算分类正确的样本百分数，可以得正确率。

## 正则化的逻辑回归

调用函数plot_data可视化第二组数据。

![](/images/blog/20180522/3.png)

特征变换：

创建更多的特征是充分挖掘数据中的信息的一种有效手段。该函数map_feature中，将数据映射为6阶多项式的所有项。

<center>\[\text{map_feature}(\pmb{x}) = \begin{bmatrix} 1\\ x_1\\ x_2 \\ x_1^2 \\ x_1 x_2 \\x_2^2 \\ x_1^3 \\ \vdots \\ x_1 x_2^5 \\ x_2^6 \end{bmatrix}\]</center>

逻辑回归的代价函数为：

<center>\[J(\theta) = \frac{1}{m} \sum_{i=1}^{m} [ -y^{(i)} \log ( h_{\theta}(x^{(i)}) ) - (1-y^{(i)}) \log ( 1-h_{\theta}(x^{(i)}) ) ] + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_{j}^{2}\]</center>

对应的梯度向量各分量为：

<center>\[\begin{split}
\frac{\partial J(\theta)}{\partial \theta_{0}} &= \frac{1}{m} \sum_{i=1}^{m} ( h_{\theta}(x^{(i)}) - y^{(i)} ) x_{0}^{(i)} \qquad \qquad \text{for } j=0 \\
\frac{\partial J(\theta)}{\partial \theta_{j}} &= \frac{1}{m} \sum_{i=1}^{m} ( h_{\theta}(x^{(i)}) - y^{(i)} ) x_{j}^{(i)} + \frac{\lambda}{m} \theta_{j} \qquad \text{for } j \geq 1
\end{split}\]</center>

如果将参数 初始化为全零值，相应的代价函数约为0.693,。可以使用与前述无正则化项类似的方法实现梯度下降，获得优化后的参数<span>$\theta^{*}$</span>。

可以调用plot_decision_boundary函数来查看最终得到的分类面。建议你调整正则化项的系数，分析正则化对分类面的影响。


![](/images/blog/20180522/4.png)

