---
layout: post
title: "吴恩达机器学习笔记"
date: 2018-05-25
categories: 机器学习 Python
tags: 机器学习 Python
author: Quan Zhang
---

* content
{:toc}

## 引言

### 无监督学习

#### 回归问题

对于房价的预测，可以采用直线拟合和二次方程拟合等方式，离散数据拟合为连续数据，是属于回归问题。

![](/images/blog/20180525/1.jpg)

#### 分类问题

对于判断肿瘤是否为良性，良性为0，恶性为1，或者良性为0，还有1类恶性，2类恶性，3类恶性等等，这属于分类问题。

![](/images/blog/20180525/2.jpg)

#### SVM支持向量机

在其他一些机器学习问题中，可能会遇到不止一种特征。举个例子，我们不仅知道肿瘤的尺寸，还知道对应患者的年龄。在其他机器学习问题中，我们通常有更多的特征，比如肿块的密度、肿瘤细胞的尺寸的一致性和形状的一致性等等。

这类多特征的问题可以用SVM支持向量机来处理。

![](/images/blog/20180525/3.jpg)

### 无监督学习

监督学习的数据集都已经被标明是阴性或阳性。所以对监督学习里的每条数据，我们已经清楚地知道，训练集对应的正确答案。然而，在无监督学习中没有任何标签。

![](/images/blog/20180525/4.jpg)

![](/images/blog/20180525/5.jpg)

针对数据集，无监督学习就能判断出数据有两个不同的聚集簇，这种算法叫做聚类算法。

## 单变量线性回归

### 模型表示

单变量线性回归，即Linear Regression with One Variable。

我们将用来描述回归问题的变量定义如下：

- $m$代表训练集中实例的数量
- $x$代表特征/输入变量
- $y$代表目标变量/输出变量
- $(x,y)$代表训练集中的实例
- $(x^{(i)},y^{(i)})$代表第$i$个观察实例
- $h$代表学习算法的解决方案或假设模型

因为只含有一个特征/输入变量，所以单变量线性回归问题可表示为：

<center>\[h_{\theta}(x)=\theta_{0}+\theta_{1}x\]</center>

### 代价函数

我们选择的`参数`$\theta_{0}$和$\theta_{1}$决定了训练集的准确程度，模型所预测的值与实际值之间的差距就是`建模误差`。

![](/images/blog/20180525/6.jpg)

我们的目标就是寻找可以使得建模误差的平方和最小的模型参数。代价函数为：

<center> \[J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \big( h_{\theta}(x^{(i)}) - y^{(i)} \big) ^{2}\] </center>

绘制一个等高线图，三个坐标分别为$\theta_{0}$和$\theta_{1}$以及$J(\theta_{0},\theta_{1})$:

![](/images/blog/20180525/7.jpg)

则可以看出三维空间中存在一个使得代价最小的点。

### 梯度下降

梯度下降的思想是：开始时我们随机选择一个参数的组合$(\theta_{0},\theta_{1},...,\theta_{n})$，计算代价函数，然后我们寻找下一个能让代价函数南湖值下降最多的参数组合。我们持续这么做直到得到一个局部最小值。因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的是否是全局最小值。

![](/images/blog/20180525/8.jpg)

梯度下降算法公式为：

![](/images/blog/20180525/9.jpg)

其中$\alpha$为学习率，他决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。

*注意：*先计算$\theta_{0}$和$\theta_{1}$，再同时更新。

## 多变量线性回归

多变量线性回归，即Linear Regression with Multiple Variable。

### 多维特征

![](/images/blog/20180525/10.jpg)

对于多维特征，我么引入一些新的说明：

- $n$代表特征的数量
- $x^{(i)}$代表第$i$个训练实例，是特征矩阵中的第$i$行。
- $x_{j}^{(i)}$代表特征矩阵中的第$i$行的第$j$个特征。

如上图的$x_{2}^{(2)}=3$，$x_{3}^{(2)}=2$。多变量的假设$h$表示为：

<center>\[h_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+...+\theta_{n}x_{n}\]</center>

为了使公式简化，令$x_{0}=1$：

<center>\[h_{\theta}(x)=\theta^{T}X\]</center>

### 多变量梯度下降

#### 代价函数

在多变量线性回归中，我们也构建一个代价函数：

<center>\[J(\theta_{0},\theta_{1}...\theta_{n})=\frac{1}{2m} \sum_{i=1}^{m} \big( h_{\theta}(x^{(i)}) - y^{(i)} \big) ^{2}\]</center>

#### 梯度下降

在多变量线性回归中，梯度下降算法与单变量类似，只不过$(j=0,1,...,n)$。

#### 特征缩放

在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助算法更快的收敛。

以房价问题为例，假设我们使用两个特征，房屋尺寸和房间的数量，尺寸的值为0-2000平方英尺，而房间数量的值则为0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图，能看出图像很扁，梯度下降算法需要非常多次的迭代才能收敛。

![](/images/blog/20180525/11.jpg)

解决办法是尝试将所有特征的尺度都尽量缩放到-1到1之间，如图：

![](/images/blog/20180525/12.jpg)

最简单的方法是令$x_{n}=\frac{x_{n}-\mu_{n}}{s_{n}}$，其中$\mu_{n}$是标准差。

#### 学习率

梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时区域收敛。

也有一些自动测试是否收敛的方法，例如将代价函数的变化值与一个阈值进行比较。

#### 特征和多项式回归

线性回归并不适用于所有的数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型或者三次方模型。

<center>\[h_{theta}=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}^{2}+\theta_{3}x_{3}^{3}\]</center>

![](/images/blog/20180525/13.jpg)

**注：**如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。

#### 正规方程

都目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程方法是更好的解决方案。如：

![](/images/blog/20180525/14.jpg)

正规方程时通过偏导为0来使得代价函数最小。

假设我们的训练集特征矩阵为$X$(包含了$x_{0}=1$)，并且我们的训练集结果为向量$y$,，则利用正规方程解出向量$\theta=(X^{T}X^{-1}X^{T}y)$。

![](/images/blog/20180525/15.jpg)

![](/images/blog/20180525/16.jpg)

运用正规方程方法求解参数：

![](/images/blog/20180525/17.jpg)

**注：**对于不可逆的矩阵，正规方程方法是不可用的。

|---|---|
|**梯度下降**|**正规方程**|
|需要选择学习率$\alpha$|不需要|
|需要多次迭代|一次运算得出|
|当特征数量n大时也适用|求逆算法大|
|适用于各种模型|只适合线性，不适合逻辑回归等其他|

**注：**只要特征数量n小于10000，通常使用正规方程的方法。对于特定的线性方程，更快。

对于不可逆的情况，可以使用正则化的方法，通过删除某些特征(线性相关的特征)，来解决不可逆的问题。

## 逻辑回归

### 分类问题

逻辑回归：输出值永远在0-1之间，即：$0 \le h_{\theta(x)} \le 1$。

### 假说表示

![](/images/blog/20180525/18.jpg)

对于线性回归来说，当$h_{\theta} \ge 0.5$时，$y=1$，否则$y=0$。当一个$x$轴特征比较大的反例，则会被预测为良性，所以线性回归不适合解决这类问题。

逻辑回归模型：

<center>\[h_{\theta}(x)=g(\theta^{T}X)\]</center>

其中，X代表特征向量，$g$代表逻辑函数sigmoid函数，$g(z)=\frac{1}{1+e^{-z}}$。

![](/images/blog/20180525/19.jpg)

### 判定边界

假设有一模型：

![](/images/blog/20180525/20.jpg)

参数$\theta$为向量[-3 1 1]，则当$-3+x_{1}+x_{2} \ge 0$时，模型输出1。我们可以绘制直线$x_{1}+x_{2}=3$，这条直线便是分界线。

![](/images/blog/20180525/21.jpg)

对于如下情况，需要采用二次方程等模型：

<center>\[h_{\theta}(x)=g(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{1}^{2}+\theta_{4}x_{2}^{2})\]</center>

![](/images/blog/20180525/22.jpg)

### 代价函数

我们将$h_{\theta}(x)=\frac{1}{1+e^{-\theta^{T}X}}$带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数：

![](/images/blog/20180525/23.jpg)

这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。

我们定义逻辑回归的代价函数为：

<center>\[J(\theta)=\frac{1}{m} \sum_{i=1}^{m} Cost(h_{\theta}(x^{(i)},y^{(i)}))\]</center>

![](/images/blog/20180525/24.jpg)

$h_{\theta}(x)$与$Cost(h_{\theta}(x),y)$之间的关系如下：

![](/images/blog/20180525/25.jpg)

得到逻辑回归的代价函数：

<center>\[J(\theta) = \frac{1}{m} \sum_{i=1}^{m} [ -y^{(i)} \log ( h_{\theta}(x^{(i)}) ) - (1-y^{(i)}) \log ( 1-h_{\theta}(x^{(i)}) ) ]\]</center>

**注：**逻辑回归的假设函数与线性回归的假设函数不一样：

1. 线性回归：$h_{\theta}(x)=\theta_{0}x_{0}+\theta_{1}x_{1}+...$

2. 逻辑函数：$h_{\theta}(x)= \frac{1}{1+e^{-\theta^{T}X}}$

### 高级优化

梯度下降并不是我们可以使用的唯一算法，比如共轭梯度算法、BFGS(变尺度法)和L-BFGS(限制变尺度法)。

### 多类别分类：一对多

对于一个多类分类问题，我们的数据集或许看起来像这样：

![](/images/blog/20180525/26.jpg)

首先假设三角形为正样本，可以得到一个逻辑回归分类；然后把正方形作为正样本又可以得到一个分类；再把叉叉作为正样本，又可以得到一个。

最后得到的模型简单记为：

<center>\[h_{\theta}^{i}(x)=p(y=i|x;\theta)\]</center>

其中，$i=(1,2,3)$。

我们要做的就是在三个分类器里面输入一个$x$，选择一个让$h_{\theta}^{i}(x)$最大的$i$。

## 正则化

### 过拟合

对于下面的回归问题，第一个模型是一个线性模型，属于 `欠拟合` 。第三个模型是一个四次方的模型，过度强调拟合原始数据，而丢失了算法的本质，预测新数据表现差，属于 `过拟合` 。而中间的模型似乎最合适。

![](/images/blog/20180525/27.jpg)

对于多项式的理解，次数越高，拟合的越好，但相应的预测能力就可能变差。

对于分类问题也存在这样的问题：

![](/images/blog/20180525/28.jpg)

**解决过拟合问题：**

1. 丢弃一些不能帮助我们正确预测的特征，可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙(如：PCA)。

2. 正则化：保留所有的特征，但是减少参数的大小。

### 代价函数

假设上面的回归问题模型为：$h_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+...$。我们可以从之前的事例可以看出，正是那些高次项导致了过拟合，所以如果我们能让这些高次项的系数接近0的话，我们就能很好的拟合了。

所以我们要做的就是在一定程度上减小这些参数$\theta$的值，这就是正则化的基本思想。我们要减小$\theta_{3}$和$\theta_{4}$的大小，我们要做的就是修改代价函数，在$\theta_{3}$和$\theta_{4}$设置一点惩罚，并最终导致选择较小的$\theta_{3}$和$\theta_{4}$，修改过的代价函数如下：

<center>\[\min_{\theta} \frac {1}{2m}[\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+10\theta_{3}^{2}+100\theta_{4}^{2}]\]</center>

假如我们有非常多的特征，并不知道其中哪些特征需要惩罚，我们将对所有的特征进行惩罚：

<center>\[J(\theta) = \frac {1}{2m}[\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda \sum_{j=1}^{n}\theta_{j}^{2}]\]</center>

其中，$\lambda称为正则化系数$。一般不对$\theta_{0}$进行惩罚，正则化的模型与原模型比较为：

![](/images/blog/20180525/29.jpg)

如果选择的参数过$\lambda$过大，则会把所有的参数都最小化，则模型变为$h_{\theta}(x)=\theta_{0}$，如图中棕色线所示。

### 正则化线性回归

对于线性回归的求解没我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程。

正则化的线性回归的代价函数为：

<center>\[J(\theta) = \frac {1}{2m}[\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda \sum_{j=1}^{n}\theta_{j}^{2}]\]</center>

梯度下降法如下：

<center>\[\theta_{0}:=\theta_{0}-a\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}\]</center>

<center>\[\theta_{j}:=\theta_{j}-a\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}+\frac{\lambda}{m}\theta_{j}\]</center>

正规方程法如下：

![](/images/blog/20180525/30.jpg)

图中的矩阵尺寸为(n+1)*(n+1)

### 正则化逻辑回归

针对逻辑回归，我们之前学了两种优化算法：梯度下降和高级的优化算法。

正则化的代价函数：

<center>\[J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \Big[ -y^{(i)} \log \big( h_{\theta}(x^{(i)}) \big) - (1-y^{(i)}) \log \big( 1-h_{\theta}(x^{(i)}) \big) \Big] + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_{j}^{2}\]</center>

正则化的梯度：

<center>\[\begin{split}
\frac{\partial J(\theta)}{\partial \theta_{0}} &= \frac{1}{m} \sum_{i=1}^{m} \big( h_{\theta}(x^{(i)}) - y^{(i)} \big) x_{0}^{(i)} \qquad \qquad \text{for } j=0 \\
\frac{\partial J(\theta)}{\partial \theta_{j}} &= \frac{1}{m} \sum_{i=1}^{m} \big( h_{\theta}(x^{(i)}) - y^{(i)} \big) x_{j}^{(i)} + \frac{\lambda}{m} \theta_{j} \qquad \text{for } j \geq 1
\end{split}\]</center>

**注：**参数$\theta_{0}$的更新规则与其他情况不同。
